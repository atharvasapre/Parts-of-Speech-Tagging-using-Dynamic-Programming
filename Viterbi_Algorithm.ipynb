{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algorithms_Project",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBO0fygwo43Z"
      },
      "outputs": [],
      "source": [
        "# Importing packages and loading in the data set \n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HMM:\n",
        "\n",
        "\n",
        "  def __init__(self,training,voc, test_corpus, test):\n",
        "\n",
        "    # Punctuation characters\n",
        "    self.punct = set(string.punctuation)\n",
        "\n",
        "    # Morphology rules used to assign unknown word tokens\n",
        "    self.noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "    self.verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "    self.adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "    self.adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "\n",
        "    with open(training, 'r') as f:\n",
        "      training_corpus = f.readlines()\n",
        "\n",
        "    with open(voc, 'r') as f:\n",
        "      voc_l = f.read().split('\\n')\n",
        "\n",
        "    # vocab: dictionary that has the index of the corresponding words\n",
        "    self.vocab = {} \n",
        "\n",
        "    # Get the index of the corresponding words. \n",
        "    for i, word in enumerate(sorted(voc_l)): \n",
        "        self.vocab[word] = i   \n",
        "\n",
        "    #Reading the test corpus\n",
        "    with open(test_corpus, 'r') as f:\n",
        "      self.y = f.readlines()\n",
        "\n",
        "\n",
        "\n",
        "    #corpus without tags, preprocessed\n",
        "    self._, self.prep = self.preprocess(self.vocab, test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    self.cnt = 0\n",
        "    for k,v in self.vocab.items():\n",
        "        self.cnt += 1\n",
        "        if self.cnt > 20:\n",
        "            break\n",
        "\n",
        "    #create dictionaries\n",
        "    self.emission_counts, self.transition_counts, self.tag_counts = self.create_dictionaries(training_corpus, self.vocab)\n",
        "\n",
        "    # get all the POS states\n",
        "    self.states = sorted(self.tag_counts.keys())\n",
        "\n",
        "\n",
        "    #create transition matrix\n",
        "    alpha = 0.001\n",
        "    self.A = self.create_transition_matrix(alpha, self.tag_counts, self.transition_counts)\n",
        "\n",
        "\n",
        "    #creating emission matrix\n",
        "    self.B = self.create_emission_matrix(alpha, self.tag_counts, self.emission_counts, list(self.vocab))\n",
        "\n",
        "    #initilaize best_probs and best_paths matrices\n",
        "    self.best_probs, self.best_paths = self.initialize(self.states, self.tag_counts, self.A, self.B, self.prep, self.vocab)\n",
        "\n",
        "    # this will take a few minutes to run => processes ~ 30,000 words VITERBI FWD\n",
        "    self.best_probs, self.best_paths = self.viterbi_forward(self.A, self.B, self.prep, self.best_probs, self.best_paths, self.vocab)\n",
        "\n",
        "\n",
        "\n",
        "    #RUnning Viterbi Backward\n",
        "    self.pred = self.viterbi_backward(self.best_probs, self.best_paths, self.prep, self.states)\n",
        "\n",
        "  def get_word_tag(self,line, vocab): \n",
        "      if not line.split():\n",
        "          word = \"--n--\"\n",
        "          tag = \"--s--\"\n",
        "          return word, tag\n",
        "      else:\n",
        "          word, tag = line.split()\n",
        "          if word not in vocab: \n",
        "              word = self.assign_unk(word)\n",
        "          return word, tag\n",
        "      return None \n",
        "\n",
        "\n",
        "  def preprocess(self, vocab, data_fp):\n",
        "      '''\n",
        "      Input: \n",
        "          data_fp: file pointer to test data\n",
        "          vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "          \n",
        "      Output: \n",
        "          orig: original data with words and the assigned POS tags\n",
        "          prep: Data without the POS tags for testing\n",
        "      '''\n",
        "      orig = []\n",
        "      prep = []\n",
        "\n",
        "      with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "          for cnt, word in enumerate(data_file):\n",
        "              if not word.split():\n",
        "                  orig.append(word.strip())\n",
        "                  word = \"--n--\"\n",
        "                  prep.append(word)\n",
        "                  continue\n",
        "              elif word.strip() not in vocab:\n",
        "                  orig.append(word.strip())\n",
        "                  word = self.assign_unk(word)\n",
        "                  prep.append(word)\n",
        "                  continue\n",
        "              else:\n",
        "                  orig.append(word.strip())\n",
        "                  prep.append(word.strip())\n",
        "\n",
        "      assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "      assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "      return orig, prep\n",
        "\n",
        "\n",
        "  def assign_unk(self, tok):\n",
        "     #Assign unk tags\n",
        "      if any(char.isdigit() for char in tok):\n",
        "          return \"--unk_digit--\"\n",
        "      elif any(char in self.punct for char in tok):\n",
        "          return \"--unk_punct--\"\n",
        "      elif any(char.isupper() for char in tok):\n",
        "          return \"--unk_upper--\"\n",
        "      elif any(tok.endswith(suffix) for suffix in self.noun_suffix):\n",
        "          return \"--unk_noun--\"\n",
        "      elif any(tok.endswith(suffix) for suffix in self.verb_suffix):\n",
        "          return \"--unk_verb--\"\n",
        "      elif any(tok.endswith(suffix) for suffix in self.adj_suffix):\n",
        "          return \"--unk_adj--\"\n",
        "      elif any(tok.endswith(suffix) for suffix in self.adv_suffix):\n",
        "          return \"--unk_adv--\"\n",
        "      return \"--unk--\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def create_dictionaries(self, training_corpus, vocab):\n",
        "      '''\n",
        "      Input: \n",
        "          prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "          training_corpus: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "          vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "          \n",
        "      Output: \n",
        "          emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "          transition_counts: a dictionary where the keys are (prev_tag,curr_tag) tuples and the value is the count\n",
        "          tag_counts: a dictionary where the keys are tags and the value is the count\n",
        "      '''    \n",
        "      emission_counts = defaultdict(int)\n",
        "      transition_counts = defaultdict(int)\n",
        "      tag_counts = defaultdict(int)\n",
        "      \n",
        "      prev_tag = '--s--' \n",
        "\n",
        "      i = 0 \n",
        "\n",
        "      for word_tag in training_corpus:\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          if i % 50000 == 0:\n",
        "              print(f\"word count = {i}\")\n",
        "          word,tag = self.get_word_tag(word_tag,vocab)\n",
        "          transition_counts[(prev_tag,tag)] += 1\n",
        "          emission_counts[(tag,word)] += 1\n",
        "          tag_counts[tag] += 1\n",
        "          prev_tag = tag\n",
        "          \n",
        "      return emission_counts, transition_counts, tag_counts\n",
        "\n",
        "  def create_transition_matrix(self, alpha, tag_counts, transition_counts):\n",
        "      ''' \n",
        "      Input: \n",
        "          alpha: number used for smoothing\n",
        "          tag_counts: a dictionary mapping each tag to its respective count\n",
        "          transition_counts: transition count for the previous word and tag\n",
        "      Output:\n",
        "          A: matrix of dimension (num_tags,num_tags)\n",
        "      '''\n",
        "      # Write your code here\n",
        "      tags=sorted(tag_counts.keys())\n",
        "      number=len(tags)\n",
        "      A=np.zeros((number,number))\n",
        "      prev_keys=set(transition_counts.keys())\n",
        "      for i in range(number):\n",
        "        for j in range(number):\n",
        "          c=0\n",
        "          k=(tags[i],tags[j])\n",
        "          if k in transition_counts:\n",
        "            c=transition_counts[k]\n",
        "          prev_tag_count=tag_counts[tags[i]]\n",
        "\n",
        "          A[i,j]=(c + alpha) / (prev_tag_count + alpha * number)\n",
        "\n",
        "      return A\n",
        "\n",
        "  def create_emission_matrix(self, alpha, tag_counts, emission_counts, vocab):\n",
        "      '''\n",
        "      Input: \n",
        "          alpha: tuning parameter used in smoothing \n",
        "          tag_counts: a dictionary mapping each tag to its respective count\n",
        "          emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "          vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "      Output:\n",
        "          B: a matrix of dimension (num_tags, len(vocab))\n",
        "      '''\n",
        "      # Write your code here\n",
        "      number=len(tag_counts)\n",
        "      tags=sorted(tag_counts.keys())\n",
        "      tot_word_ct=len(vocab)\n",
        "      B = np.zeros((number, tot_word_ct))\n",
        "\n",
        "\n",
        "      keys=set(list(emission_counts.keys()))\n",
        "\n",
        "      for i in range(number):\n",
        "        for j in range(tot_word_ct):\n",
        "          c=0\n",
        "          key=(tags[i],vocab[j])\n",
        "          if key in emission_counts.keys():\n",
        "            c=emission_counts[key]\n",
        "          pos_tag_ct= tag_counts[tags[i]]\n",
        "\n",
        "          B[i,j] = (c + alpha) / (pos_tag_ct + alpha * tot_word_ct)\n",
        "\n",
        "\n",
        "      return B\n",
        "\n",
        "  def initialize(self, states, tag_counts, A, B, corpus, vocab):\n",
        "    #helper function to initialize the matrix\n",
        "    no_of_tags = len(tag_counts)\n",
        "    best_probs = np.zeros((len(tag_counts), len(corpus)))\n",
        "    best_paths= np.zeros((len(tag_counts), len(corpus)))\n",
        "\n",
        "    s_idx = states.index(\"--s--\")\n",
        "    for i in range(no_of_tags):\n",
        "      if A[s_idx,i]==0:\n",
        "        best_probs[i,0]=float('-inf')\n",
        "      else:\n",
        "        best_probs[i,0]=math.log(A[s_idx,i])+math.log(B[i,vocab[corpus[0]]])\n",
        "    return best_probs, best_paths\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def viterbi_forward(self, A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "      '''\n",
        "      Input: \n",
        "          A, B: The transiton and emission matrices respectively\n",
        "          test_corpus: a list containing a preprocessed corpus\n",
        "          best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "          best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "          vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "      Output: \n",
        "          best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "          best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "      '''\n",
        "      # Write your code here\n",
        "      num_tags = best_probs.shape[0]\n",
        "      for i in range(1,len(test_corpus)):\n",
        "        for j in range(num_tags):\n",
        "          best_prob_i = float('-inf')\n",
        "          best_path_i = None\n",
        "          for k in range(num_tags):\n",
        "            prob = best_probs[k,i-1] + math.log(A[k,j]) + math.log(B[j,vocab.get(test_corpus[i])])\n",
        "            if prob>best_prob_i:\n",
        "              best_prob_i = prob\n",
        "              best_path_i = k\n",
        "          best_probs[j,i] = best_prob_i\n",
        "          best_paths[j,i] = best_path_i \n",
        "      return best_probs, best_paths\n",
        "\n",
        "\n",
        "\n",
        "  def viterbi_backward(self, best_probs, best_paths, corpus, states):\n",
        "    '''\n",
        "      Input: \n",
        "          corpus: a list containing a preprocessed corpus\n",
        "          best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "          best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "          states: a list of all possible POS tags\n",
        "      Output: \n",
        "          best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "          best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "    m = best_paths.shape[1]\n",
        "    tracker = [None] * m\n",
        "    num_tags = best_probs.shape[0]\n",
        "    best_p = -9999999999999 \n",
        "    pos_pred = [None] * m\n",
        "    for k in range(1,num_tags-1):\n",
        "      if best_probs[k,m-1]<best_probs[k-1,m-1]:\n",
        "        best_p=best_probs[k-1,m-1]\n",
        "        tracker[m - 1] = int(k)\n",
        "    pos_pred[m-1] = states[k]\n",
        "    for i in range(m-1, 0, -1):\n",
        "      tracker[i-1]=best_paths[int(tracker[i]), i]\n",
        "      pos_pred[i-1] = states[int(tracker[i-1])]\n",
        "    return pos_pred\n"
      ],
      "metadata": {
        "id": "lifSCbn2qS2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "        # Split the label into the word and the POS tag\n",
        "        word_tag_tuple = y.split()\n",
        "        \n",
        "        # Check that there is actually a word and a tag\n",
        "        # no more and no less than 2 items\n",
        "        if len(word_tag_tuple)!=2: # complete this line\n",
        "            continue \n",
        "\n",
        "        # store the word and tag separately\n",
        "        word, tag = word_tag_tuple\n",
        "        \n",
        "        # Check if the POS tag label matches the prediction\n",
        "        if prediction == tag: # complete this line\n",
        "            \n",
        "            # count the number of times that the prediction\n",
        "            # and label match\n",
        "            num_correct += 1\n",
        "            \n",
        "        # keep track of the total number of examples (that have valid labels)\n",
        "        total += 1\n",
        "        \n",
        "    p=(num_correct/total)*100\n",
        "    return print(\"The accuracy of the Viterbi Algorithm on test dataset is \"+str(np.round(p,3))+\"%\")"
      ],
      "metadata": {
        "id": "v1aj4afLxnK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj= HMM(training=\"WSJ-2_21.pos\", voc= \"hmm_vocab.txt\", test_corpus=\"WSJ-24.pos\", test=\"test.words.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOBKqVBe5zyQ",
        "outputId": "a27d3d28-8430-41db-debd-116acde48823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n",
            "word count = 400000\n",
            "word count = 450000\n",
            "word count = 500000\n",
            "word count = 550000\n",
            "word count = 600000\n",
            "word count = 650000\n",
            "word count = 700000\n",
            "word count = 750000\n",
            "word count = 800000\n",
            "word count = 850000\n",
            "word count = 900000\n",
            "word count = 950000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(obj.pred, obj.y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEmfYyixnbOV",
        "outputId": "947394eb-47f6-478b-b122-c1c9af46f20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the Viterbi Algorithm on test dataset is 95.306%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Visualization:**"
      ],
      "metadata": {
        "id": "VBweGAG8_gp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try viewing emissions using sample words\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "\n",
        "# Get the integer ID for each word\n",
        "cols = [obj.vocab[a] for a in cidx]\n",
        "\n",
        "# Choose POS tags to show in a sample dataframe\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "\n",
        "# For each POS tag, get the row number from the 'states' list\n",
        "rows = [obj.states.index(a) for a in rvals]\n",
        "\n",
        "# Get the emissions for the sample of words, and the sample of POS tags\n",
        "B_sub = pd.DataFrame(obj.B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySzn3tSN_982",
        "outputId": "5f433665-8526-4541-ca29-6f5cbc42cd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
            "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
            "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
            "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
            "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
            "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing transition matrix\n",
        "print(f\"A at row 0, col 0: {obj.A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {obj.A[3,1]:.4f}\")\n",
        "\n",
        "print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(obj.A[30:35,30:35], index=obj.states[30:35], columns = obj.states[30:35] )\n",
        "print(A_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPFsZb26ATAe",
        "outputId": "c7156c5f-e7fe-4d52-b405-b4716e9dee88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A at row 0, col 0: 0.000007040\n",
            "A at row 3, col 1: 0.1691\n",
            "View a subset of transition matrix A\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
            "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
            "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
            "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
            "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing a subset of the best_probs matrix\n",
        "print(\"View a subset of best_prob matrix A\")\n",
        "A_sub = pd.DataFrame(obj.best_probs[30:35,30:35], index=obj.states[30:35], columns = cidx )\n",
        "print(A_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNTmnVzyAnT2",
        "outputId": "a0e6a75a-9a4f-45fa-a4b1-62610d8ab55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View a subset of best_prob matrix A\n",
            "            725    adroitly   engineers    promoted     synergy\n",
            "RBS -202.756188 -208.388385 -210.469384 -210.159431 -223.792237\n",
            "RP  -202.582976 -217.722668 -207.237257 -215.529735 -224.139572\n",
            "SYM -202.008781 -214.230938 -217.410216 -220.737687 -222.033388\n",
            "TO  -200.440161 -209.469378 -209.069517 -216.222978 -221.096697\n",
            "UH  -208.741895 -214.620888 -209.793465 -213.526235 -228.704175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing a subset of the best_paths matrix\n",
        "print(\"View a subset of best_prob matrix A\")\n",
        "A_sub = pd.DataFrame(obj.best_paths[30:35,30:35], index=obj.states[30:35], columns = cidx )\n",
        "print(A_sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apC6Zcyk1SGS",
        "outputId": "212c4f13-620e-4eb2-8cfd-86906e21b5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View a subset of best_prob matrix A\n",
            "      725  adroitly  engineers  promoted  synergy\n",
            "RBS  20.0      19.0       35.0      11.0     21.0\n",
            "RP   20.0      19.0       35.0      11.0     21.0\n",
            "SYM  20.0      19.0       35.0      11.0     21.0\n",
            "TO   20.0      19.0       35.0      11.0     21.0\n",
            "UH   35.0      19.0       35.0      11.0     34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking true and predicted POS tag values for first test sentence\n",
        "print(\"Predicted POS sequence: \",obj.pred[0:13])\n",
        "print(\"True POS sequence: \",[\"\".join(reversed(i[-2:-4:-1])) for i in obj.y[0:13]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQ1iXtX4E-7",
        "outputId": "f11fe0b7-37d8-4395-95b3-d1324d6de88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted POS sequence:  ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NN', 'VBZ', 'DT', 'NN']\n",
            "True POS sequence:  ['DT', 'NN', 'OS', 'NN', 'MD', 'VB', 'BN', 'IN', 'JJ', 'NN', 'NS', 'DT', 'NN']\n"
          ]
        }
      ]
    }
  ]
}
